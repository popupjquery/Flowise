{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/popupjquery/Flowise/blob/main/gpt_prompt_engineer_Classification_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# gpt-prompt-engineer -- Classification Version\n",
        "By Matt Shumer (https://twitter.com/mattshumer_)\n",
        "\n",
        "Github repo: https://github.com/mshumer/gpt-prompt-engineer\n",
        "\n",
        "Generate an optimal prompt for a given classification task that can be evaluated with 'true'/'false' outputs.\n",
        "\n",
        "You just need to describe the task clearly, and provide some test cases (for example, if we're classifying statements as 'happy' or not, a 'true' test case could be \"I had a great day!\", and a 'false' test case could be \"I am feeling gloomy.\").\n",
        "\n",
        "To generate a prompt:\n",
        "1. In the first cell, add in your OpenAI key.\n",
        "2. If you don't have GPT-4 access, change `model='gpt-4'` in the second cell to `model='gpt-3.5-turbo'`. If you do have access, skip this step.\n",
        "2. In the last cell, fill in the description of your task, as many test cases as you want (test cases are example prompts and their expected output), and the number of prompts to generate.\n",
        "3. Run all the cells! The AI will generate a number of candidate prompts, and test them all to find the best one!"
      ],
      "metadata": {
        "id": "L0Ey7JZ5iLo1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW3ztLRsolnk"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install prettytable\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "import time\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"ADD YOUR KEY HERE\" # enter your OpenAI API key here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_candidate_prompts(description, test_cases, number_of_prompts):\n",
        "  outputs = openai.ChatCompletion.create(\n",
        "      model='gpt-4',\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"\"\"Your job is to generate system prompts for GPT-4, given a description of the use-case and some test cases.\n",
        "\n",
        "The prompts you will be generating will be for classifiers, with 'true' and 'false' being the only possible outputs.\n",
        "\n",
        "In your generated prompt, you should describe how the AI should behave in plain English. Include what it will see, and what it's allowed to output. Be creative in with prompts to get the best possible results. The AI knows it's an AI -- you don't need to tell it this.\n",
        "\n",
        "You will be graded based on the performance of your prompt... but don't cheat! You cannot include specifics about the test cases in your prompt. Any prompts with examples will be disqualified.\n",
        "\n",
        "Most importantly, output NOTHING but the prompt. Do not include anything else in your message.\"\"\"},\n",
        "          {\"role\": \"user\", \"content\": f\"Here are some test cases:`{test_cases}`\\n\\nHere is the description of the use-case: `{description.strip()}`\\n\\nRespond with your prompt, and nothing else. Be creative.\"}\n",
        "          ],\n",
        "      temperature=.9,\n",
        "      n=number_of_prompts)\n",
        "\n",
        "  prompts = []\n",
        "\n",
        "  for i in outputs.choices:\n",
        "    prompts.append(i.message.content)\n",
        "  return prompts"
      ],
      "metadata": {
        "id": "KTRFiBhSouz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_candidate_prompts(test_cases, prompts):\n",
        "  prompt_results = {prompt: {'correct': 0, 'total': 0} for prompt in prompts}\n",
        "\n",
        "  # Initialize the table\n",
        "  table = PrettyTable()\n",
        "  table.field_names = [\"Prompt\", \"Expected\"] + [f\"Prompt {i+1}-{j+1}\" for j, prompt in enumerate(prompts) for i in range(prompts.count(prompt))]\n",
        "\n",
        "\n",
        "  # Wrap the text in the \"Prompt\" column\n",
        "  table.max_width[\"Prompt\"] = 100\n",
        "\n",
        "\n",
        "  for test_case in test_cases:\n",
        "      row = [test_case['prompt'], test_case['answer']]\n",
        "      for prompt in prompts:\n",
        "          x = openai.ChatCompletion.create(\n",
        "              model='gpt-3.5-turbo',\n",
        "              messages=[\n",
        "                  {\"role\": \"system\", \"content\": prompt},\n",
        "                  {\"role\": \"user\", \"content\": f\"{test_case['prompt']}\"}\n",
        "              ],\n",
        "              logit_bias={\n",
        "                  '1904': 100,  # 'true' token\n",
        "                  '3934': 100,  # 'false' token\n",
        "              },\n",
        "              max_tokens=1,\n",
        "              temperature=0,\n",
        "          ).choices[0].message.content\n",
        "\n",
        "\n",
        "          status = \"✅\" if x == test_case['answer'] else \"❌\"\n",
        "          row.append(status)\n",
        "\n",
        "          # Update model results\n",
        "          if x == test_case['answer']:\n",
        "              prompt_results[prompt]['correct'] += 1\n",
        "          prompt_results[prompt]['total'] += 1\n",
        "\n",
        "      table.add_row(row)\n",
        "\n",
        "  print(table)\n",
        "\n",
        "  # Calculate and print the percentage of correct answers and average time for each model\n",
        "  best_prompt = None\n",
        "  best_percentage = 0\n",
        "  for i, prompt in enumerate(prompts):\n",
        "      correct = prompt_results[prompt]['correct']\n",
        "      total = prompt_results[prompt]['total']\n",
        "      percentage = (correct / total) * 100\n",
        "      print(f\"Prompt {i+1} got {percentage:.2f}% correct.\")\n",
        "      if percentage > best_percentage:\n",
        "          best_percentage = percentage\n",
        "          best_prompt = prompt\n",
        "\n",
        "  print(f\"The best prompt was '{best_prompt}' with a correctness of {best_percentage:.2f}%.\")"
      ],
      "metadata": {
        "id": "w4ltgxntszwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_cases = [\n",
        "    {\n",
        "        'prompt': 'Find the best contact email on this site.',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'who is the current president?',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'order me a pizza',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'what are some ways a doctor could use an assistant?',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'write a speech on the danger of cults',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Make a reservation at The Accent for 9pm',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'organize my google drive',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Find the highest-rated Italian restaurant near me.',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Explain the theory of relativity.',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'What are the main differences between Python and Java programming languages?',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Translate the following English sentence to Spanish: \"The weather today is great.\"',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Create a new event on my calendar for tomorrow at 2 pm.',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Write a short story about a lonely cowboy.',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Design a logo for a startup.',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Compose a catchy jingle for a new soda brand.',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Calculate the square root of 1999.',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'What are the health benefits of yoga?',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'find me a source of meat that can be shipped to canada',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Find the best-selling book of all time.',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'What are the top 5 tourist attractions in Brazil?',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'List the main ingredients in a traditional lasagna recipe.',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'How does photosynthesis work in plants?',\n",
        "        'answer': 'true'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Write a Python program to reverse a string.',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Create a workout routine for a beginner.',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Edit my resume to highlight my project management skills.',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Draft an email to a client to discuss a new proposal.',\n",
        "        'answer': 'false'\n",
        "    },\n",
        "    {\n",
        "        'prompt': 'Plan a surprise birthday party for my best friend.',\n",
        "        'answer': 'false'\n",
        "    }]\n",
        "\n",
        "\n",
        "description = \"Decide if a task is research-heavy.\" # describe the classification task clearly\n",
        "number_of_prompts = 10 # choose how many prompts you want to generate and test\n",
        "\n",
        "\n",
        "\n",
        "candidate_prompts = generate_candidate_prompts(description, test_cases, number_of_prompts)\n",
        "test_candidate_prompts(test_cases, candidate_prompts)"
      ],
      "metadata": {
        "id": "SBJEi1hkrT9T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}